{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d8abfc",
   "metadata": {},
   "source": [
    "# Web Scraping Script Documentation\n",
    "\n",
    "## Purpose\n",
    "\n",
    "This Python script has been developed as part of Task 1 of the internship project, adhering to the following guidelines:\n",
    "\n",
    "1. **Website Selection:**\n",
    "   - The script is designed to scrape data from the Wikipedia website, a publicly accessible source of information.\n",
    "\n",
    "2. **Web Scraping:**\n",
    "\n",
    "   - Utilizing the Beautiful Soup and Requests libraries, the script extracts data from various sections of the Wikipedia page, focusing on headings and their respective word counts.\n",
    "\n",
    "\n",
    "3. **Data Processing:**\n",
    "   - The script counts the number of words under each heading, providing valuable information about the content structure.\n",
    "\n",
    "4. **Automation:**\n",
    "   - Automation is achieved by scheduling the script to run every 24 hours. This ensures regular updates to the dataset.\n",
    "\n",
    "5. **Documentation:**\n",
    "   - The script includes comprehensive documentation to explain its purpose, execution steps, components, dependencies, and any additional notes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899888ef",
   "metadata": {},
   "source": [
    "### 1. Import Necessary Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "006efac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from tabulate import tabulate\n",
    "import schedule\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b5e806",
   "metadata": {},
   "source": [
    "## Imported Libraries\n",
    "\n",
    "1. **requests**: This library is used for making HTTP requests. It is commonly employed to fetch data from websites.\n",
    "\n",
    "2. **BeautifulSoup**: It is employed for parsing HTML content. BeautifulSoup facilitates the extraction of data from HTML and XML files.\n",
    "\n",
    "3. **re (regular expressions)**: Regular expressions provide a powerful and flexible means to search, match, and manipulate text. This library is used for pattern matching in strings.\n",
    "\n",
    "4. **tabulate**: This library is utilized for creating tables. It simplifies the process of formatting and displaying tabular data.\n",
    "\n",
    "5. **schedule**: The schedule library allows for task scheduling. It enables the automation of recurring tasks at specified intervals.\n",
    "\n",
    "6. **time**: This library is used for introducing delays in the script. It can be employed to control the timing of various operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ad6d14",
   "metadata": {},
   "source": [
    "### 2. Define Word Count Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e94beca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    \"\"\"\n",
    "    Function to count the number of words in a given text.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "    - int: The number of words in the text.\n",
    "    \"\"\"\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cf087c",
   "metadata": {},
   "source": [
    "The `count_words` function is designed to take a text input and utilize a regular expression to determine the number of words within the text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c5f3ae",
   "metadata": {},
   "source": [
    "### 3. Define Web Scraping Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c446ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_and_display():\n",
    "    \"\"\"\n",
    "    Function to initiate web scraping and display the word count of subheadings on the Wikipedia page.\n",
    "    \"\"\"\n",
    "    url = \"https://en.wikipedia.org/\"\n",
    "    scrapeWikiArticle(url)\n",
    "\n",
    "def scrapeWikiArticle(url):\n",
    "    \"\"\"\n",
    "    Function to perform web scraping on the provided Wikipedia URL, count words, and display the results.\n",
    "\n",
    "    Parameters:\n",
    "    - url (str): The URL of the Wikipedia page.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error making the request: {e}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    bodyContent = soup.find(id=\"bodyContent\")\n",
    "    if bodyContent:\n",
    "        subheadings = bodyContent.find_all(['h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "\n",
    "        data = []\n",
    "        for subheading in subheadings:\n",
    "            subheading_text = subheading.get_text(strip=True)\n",
    "            subheading_words = count_words(subheading_text)\n",
    "            subheading_level = subheading.name\n",
    "\n",
    "            data.append([subheading_level, subheading_text, subheading_words])\n",
    "\n",
    "        total_words = sum([row[2] for row in data])\n",
    "        data.append([\"Total\", \"\", total_words])\n",
    "\n",
    "        headers = [\"Level\", \"Subheading\", \"Number of Words\"]\n",
    "        print(tabulate(data, headers=headers, tablefmt=\"grid\"))\n",
    "    else:\n",
    "        print(\"Body content not found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e736b805",
   "metadata": {},
   "source": [
    "#  Explanation\n",
    "\n",
    "\n",
    "## `scrape_and_display` Function\n",
    "\n",
    "## `scrapeWikiArticle` Function\n",
    "\n",
    "Two functions are defined to facilitate web scraping and word count display. The scrape_and_display function sets the initial URL, and the scrapeWikiArticle function performs the actual scraping, counts the words in subheadings, and displays the results using the tabulate library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99293f17",
   "metadata": {},
   "source": [
    "### 4. Schedule the Task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9de459ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Every 2 minutes do scrape_and_display() (last run: [never], next run: 2023-12-11 18:45:29)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Schedule the task to run every 2 minutes\n",
    "schedule.every(2).minutes.do(scrape_and_display)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12ac7ce",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "This part of the script uses the schedule library to set up a recurring task. It schedules the scrape_and_display function\n",
    "to run every 2 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ed8525",
   "metadata": {},
   "source": [
    "### 5. Keep the Script Running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65860d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------------------+-------------------+\n",
      "| Level   | Subheading                    |   Number of Words |\n",
      "+=========+===============================+===================+\n",
      "| h2      | From today's featured article |                 5 |\n",
      "+---------+-------------------------------+-------------------+\n",
      "| h2      | Did you knowÂ ...              |                 3 |\n",
      "+---------+-------------------------------+-------------------+\n",
      "| h2      | In the news                   |                 3 |\n",
      "+---------+-------------------------------+-------------------+\n",
      "| h2      | On this day                   |                 3 |\n",
      "+---------+-------------------------------+-------------------+\n",
      "| h2      | From today's featured list    |                 5 |\n",
      "+---------+-------------------------------+-------------------+\n",
      "| h2      | Today's featured picture      |                 4 |\n",
      "+---------+-------------------------------+-------------------+\n",
      "| h2      | Other areas of Wikipedia      |                 4 |\n",
      "+---------+-------------------------------+-------------------+\n",
      "| h2      | Wikipedia's sister projects   |                 4 |\n",
      "+---------+-------------------------------+-------------------+\n",
      "| h2      | Wikipedia languages           |                 2 |\n",
      "+---------+-------------------------------+-------------------+\n",
      "| Total   |                               |                33 |\n",
      "+---------+-------------------------------+-------------------+\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m      3\u001b[0m     schedule\u001b[38;5;241m.\u001b[39mrun_pending()\n\u001b[1;32m----> 4\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Keep the script running to allow scheduled tasks\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66564812",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "The script enters an infinite loop, allowing the scheduled tasks to run. It uses schedule.run_pending() to check and \n",
    "execute any pending scheduled tasks and introduces a 1-second delay between iterations to avoid unnecessary CPU usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c938c0",
   "metadata": {},
   "source": [
    "# Dependencies\n",
    "\n",
    "The script relies on the following dependencies to execute successfully:\n",
    "\n",
    "1. **Python 3.x**\n",
    "\n",
    "2. **Requests Library**\n",
    "\n",
    "3. **BeautifulSoup Library**\n",
    "\n",
    "4. **Regular Expression (re) Module**\n",
    "\n",
    "5. **Tabulate Library**\n",
    "\n",
    "6. **Schedule Library**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7cec06",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "The script fetches subheadings from the Wikipedia page, counts the words, and displays the results in a tabulated format.\n",
    "Adjustments may be needed for different websites or data processing requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d664e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
